install.packages(c("REPPlab", "tourr"))
library(tourr)
data(olive)
head(olive)
X <- olive[,3:9]
# scitaju sa priblizne na 1 (na 10 000), posledny udaj nezahrnieme
n <- dim(X)[1]
k <- dim(X)[2]
n
k
X <- olive[,3:9]
# scitaju sa priblizne na 1 (na 10 000), posledny udaj nezahrnieme
n <- dim(X)[1]
k <- dim(X)[2]
# vektor farieb podla Region
farby <- rep("red",n)
farby[olive[,1]==2] <- "blue"
farby[olive[,1]==3] <- "black"
oblasti <- factor(olive[,2], label=1:9) # oblasti ako cisla
# Zakladne pohlady na data
#--------------------------
plot(X)
plot(X, col=farby, pch=16, cex=0.8)
# Nacitanie dat
#--------------
library(tourr)
data(olive)
head(olive)
# 1., 2. stlpec: kategorizacia
# 3.+ stlpce: data
X <- olive[,3:9]
# scitaju sa priblizne na 1 (na 10 000), posledny udaj nezahrnieme
n <- dim(X)[1]
k <- dim(X)[2]
# vektor farieb podla Region
farby <- rep("red",n)
farby[olive[,1]==2] <- "blue"
farby[olive[,1]==3] <- "black"
oblasti <- factor(olive[,2], label=1:9) # oblasti ako cisla
# Zakladne pohlady na data
#--------------------------
plot(X)
plot(X, col=farby, pch=16, cex=0.8)
# outlieri / outliery
plot(X[,1], X[,2], type="n")
text(X[,1], X[,2], labels=1:n)
pairs(X,
panel=function(x, y, ...) { points(x, y, type="n", ...);
text(x, y, cex=0.8, 1:n) })
plot(X)
plot(X, col=farby, pch=16, cex=0.8)
plot(X, col=farby, pch=16, cex=0.8)
plot(X, col=farby, pch=16, cex=0.8)
plot(X[,1], X[,2], type="n")
text(X[,1], X[,2], labels=1:n)
# 522 je tu outlier
pairs(X,
panel=function(x, y, ...) { points(x, y, type="n", ...);
text(x, y, cex=0.8, 1:n) })
library(REPPlab)
setwd("C:/Users/Thu Beo/Desktop/5_r/5leto/Redukcia dimenzia dat")
psych <- read.table("psychtests.csv", header=TRUE, sep=";")
head(psych)
X <- psych[,5:28] # vysledky v testoch
head(X)
psych.fa0 <- factanal(X, factors=2, method="mle", rotation="none")
print(psych.fa0)
psych.fa0$loadings
psych.fa0$uniqueness
# Viac faktorov
psych.fa <- factanal(X, factors=5, method="mle", rotation="none")
print(psych.fa)
# Varimax rotacia
varimax(psych.fa$loadings)
V <- varimax(psych.fa$loadings)$rotmat
round(psych.fa$loadings %*% V, 3)
V
# ... priamo vo factanal
psych.fa2 <- factanal(X, factors=5, method="mle", rotation="varimax")
print(psych.fa2)
# Ine startovacie body (startovacie uniqueness)
psych.fa3 <- factanal(X, factors=5, method="mle",
rotation="varimax", start=rep(1/2,24))
print(psych.fa3)
st <- c(0.51, 0.15, 0.70, 0.18, 0.72, 0.86, 0.30, 0.60, 0.02, 0.32, 0.58,
0.63, 0.79, 0.12, 0.56, 0.46, 0.30, 0.05, 0.32, 0.85, 0.17, 0.08, 0.95, 0.12)
psych.fa3b <- factanal(X, factors=5, method="mle",
rotation="varimax", start=st)
print(psych.fa3b)
psych.fa3$STATISTIC  # 'vzdialenost' modelovanych hodnot od dat
psych.fa3b$STATISTIC
# Vykreslenie
psych.fa2 <- factanal(X, factors=5, method="mle", rotation="varimax", scores="regression")
Y <- psych.fa2$scores
plot(Y[,1:2], pch=16, col=c("black","magenta")[psych$Grp], main="Dve skoly") # po skolach
# muzi, zeny
plot(Y[,1:2], pch=16, col=c("red","blue")[as.numeric(psych$Sex)])
legend( x="topright", legend=c("Zeny","Muzi"), pch=16, col=c("red","blue") )
# PCA
# Importing the dataset
dataset = read.csv('Wine.csv')
# Splitting the dataset into the Training set and Test set
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Customer_Segment, SplitRatio = 0.8)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Feature Scaling
training_set[-14] = scale(training_set[-14])
test_set[-14] = scale(test_set[-14])
# Applying PCA
# install.packages('caret')
library(caret)
# install.packages('e1071')
library(e1071)
pca = preProcess(x = training_set[-14], method = 'pca', pcaComp = 2)
training_set = predict(pca, training_set)
training_set = training_set[c(2, 3, 1)]
test_set = predict(pca, test_set)
test_set = test_set[c(2, 3, 1)]
# Fitting SVM to the Training set
# install.packages('e1071')
library(e1071)
classifier = svm(formula = Customer_Segment ~ .,
data = training_set,
type = 'C-classification',
kernel = 'linear')
# Predicting the Test set results
y_pred = predict(classifier, newdata = test_set[-3])
# Making the Confusion Matrix
cm = table(test_set[, 3], y_pred)
# Visualising the Training set results
library(ElemStatLearn)
set = training_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('PC1', 'PC2')
y_grid = predict(classifier, newdata = grid_set)
plot(set[, -3],
main = 'SVM (Training set)',
xlab = 'PC1', ylab = 'PC2',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 2, 'deepskyblue', ifelse(y_grid == 1, 'springgreen3', 'tomato')))
points(set, pch = 21, bg = ifelse(set[, 3] == 2, 'blue3', ifelse(set[, 3] == 1, 'green4', 'red3')))
# Visualising the Test set results
library(ElemStatLearn)
set = test_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('PC1', 'PC2')
y_grid = predict(classifier, newdata = grid_set)
plot(set[, -3], main = 'SVM (Test set)',
xlab = 'PC1', ylab = 'PC2',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 2, 'deepskyblue', ifelse(y_grid == 1, 'springgreen3', 'tomato')))
points(set, pch = 21, bg = ifelse(set[, 3] == 2, 'blue3', ifelse(set[, 3] == 1, 'green4', 'red3')))
setwd("C:/Users/Thu Beo/Desktop/UDEMY_COURSE/Machine Learning A-Z Hands-On Python and R In Data Science/Machine Learning A-Z (Codes and Datasets)/Part 9 - Dimensionality Reduction/Section 43 - Principal Component Analysis (PCA)/R")
# Importing the dataset
dataset = read.csv('Wine.csv')
# Splitting the dataset into the Training set and Test set
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Customer_Segment, SplitRatio = 0.8)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Feature Scaling
training_set[-14] = scale(training_set[-14])
test_set[-14] = scale(test_set[-14])
head(training_set)
head(dataset)
# Applying PCA
# install.packages('caret')
library(caret)
# install.packages('e1071')
library(e1071)
pca = preProcess(x = training_set[-14], method = 'pca', pcaComp = 2)
install.packages('caret')
install.packages('e1071')
install.packages("e1071")
library(caret)
# install.packages('e1071')
library(e1071)
pca = preProcess(x = training_set[-14], method = 'pca', pcaComp = 2)
training_set = predict(pca, training_set)
library(caret)
install.packages("princurve")
setwd("C:/Users/Thu Beo/Desktop/5_r/5leto/Redukcia dimenzia dat")
food <- read.table("food.txt", header=TRUE)
head(food)
n <- nrow(food
# data predelene vahou
X <- food[,9:14]
head(X)
# krajsie nazvy stlpcov
colnames(X) <- c("Fat", "Energy", "Carbohydrates", "Protein", "Cholesterol", "Saturated.fat")
# vyskalovanie
X <- X - tcrossprod(rep(1,n), apply(X,2,mean))  # vycentrovanie
Y <- as.matrix(X) %*% diag( 1/apply(X,2,sd) )   # disperzie na 1
colnames(Y) <- colnames(X)
# PCA
#----
P <- prcomp(Y, scale=FALSE)
plot(P$x[, 1], P$x[, 2])
biplot(P, cex=c(2,1), xlabs=rep(".", nrow(X)))
X <- food[,9:14]
head(X)
colnames(X) <- c("Fat", "Energy", "Carbohydrates", "Protein", "Cholesterol", "Saturated.fat")
# vyskalovanie
X <- X - tcrossprod(rep(1,n), apply(X,2,mean))  # vycentrovanie
Y <- as.matrix(X) %*% diag( 1/apply(X,2,sd) )   # disperzie na 1
colnames(Y) <- colnames(X)
n <- nrow(food)
# vyskalovanie
X <- X - tcrossprod(rep(1,n), apply(X,2,mean))  # vycentrovanie
Y <- as.matrix(X) %*% diag( 1/apply(X,2,sd) )   # disperzie na 1
colnames(Y) <- colnames(X)
P <- prcomp(Y, scale=FALSE)
plot(P$x[, 1], P$x[, 2])
biplot(P, cex=c(2,1), xlabs=rep(".", nrow(X)))
#=================
library(princurve)
pc <- principal_curve(Y, maxit=50)
plot(pc)            # krivka
points(pc, cex=0.8) # projekcie dat
# toto okno nezatvarat
# Vyskusajte: pre as.matrix(X)
# vykreslenie krivky cez lubovolnu dvojicu x-ov
# aj s povodnymi datami
windows()
s <- pc$s       # suradnice projekcii
ord <- pc$ord   # usporiadanie bodov na krivke
vyber <- c(1,2) # ktore 2 premenne
plot(Y[,vyber], cex=0.5, pch=19)  # povodne data
points(s[ord,vyber], cex=0.8, col="red")
lines(s[ord,vyber ], col="red")
# v 1-rozmere
lam <- pc$lambda   # vzdialenosti projekcii od zaciatku krivky
plot(lam, rep(1,n))
# vyfarbene podla napr. kalorii
plot(lam[order(Y[,2])], rep(1,n), cex=0.7, col=heat.colors(n)[n:1], main="Kalorie")
# order usporiada od najmensieho po najvacsie
# heat color treba prehodit, aby cervena bola najvyssia hodnota
# pre 1-rozmer mozeme aj vykreslit hodnoty premennych
plot(lam[order(Y[,1])], food[order(Y[,1]),9], cex=0.8, pch=19,
ylab="Tuky", col=heat.colors(n)[n:1])
# x-ova os: pozicia na P krivke
# y-ova os: podiel tukov
plot(lam[order(Y[,2])], food[order(Y[,2]),10], cex=0.8, pch=19,
ylab="Kalorie", col=heat.colors(n)[n:1])
plot(lam[order(Y[,3])], food[order(Y[,3]),11], cex=0.8, pch=19,
ylab="Uhlovodiky", col=heat.colors(n)[n:1])
plot(lam[order(Y[,4])], food[order(Y[,4]),12], cex=0.8, pch=19,
ylab="Bielkoviny", col=heat.colors(n)[n:1])
plot(lam[order(Y[,5])], food[order(Y[,5]),13], cex=0.8, pch=19,
ylab="Cholesterol", col=heat.colors(n)[n:1])
plot(lam[order(Y[,6])], food[order(Y[,6]),14], cex=0.8, pch=19,
ylab="Nasytene tuky", col=heat.colors(n)[n:1])
food <- read.table("food.txt", header=TRUE, sep=" ")
head(food)
X <- food[,9:14]   # data predelene vahou
head(X)
# kazdy riadok = jedno jedlo
# kolko tukov (kalorii, uhlovodikov, ...) obsahuje dane jedlo na 1g vahy
# krajsie nazvy stlpcov
colnames(X) <- c("Fat", "Energy", "Carbohydrates", "Protein", "Cholesterol", "Saturated.fat")
# Pohlad na data
plot(X, pch=19, cex=0.1)
round(cor(X), 3) # korelacna matica
# Manualne PCA
mX <- apply(X, 2, mean)
X <- X - matrix(mX, nrow=nrow(X), ncol=ncol(X), byrow=TRUE) # centrovanie
S <- var(X)
lam <- eigen(S)$values
print(lam)
U <- eigen(S)$vectors
print(U)
# PC1 a PC2 su viacmenej energy, cholesterol
diag(S)
# to su najviac rozptylene premenne
# Skalovane premenne
Xs <- as.matrix(X) %*% diag(1/sqrt(diag(S)))
R <- cor(X)
R - var(Xs)
lam <- eigen(R)$values; print(lam)
U <- eigen(R)$vectors; print(U)
Y <- as.matrix(Xs) %*% U
plot(Y[,1], Y[,2], pch=16, cex=0.5)
# Skalovanie a centrovanie sa da pomocou scale()
# X2 <- scale(X, center=TRUE, scale=TRUE)
# max(abs(Xs-X2)) # ... 0
# Rko: funkcia prcomp
food.pca <- prcomp(X, scale=TRUE)
print(food.pca)
# Objekt food.pca
attributes(food.pca)
print(food.pca$rotation) # vl. vektory == hl. komponenty
print(U)
print(food.pca$x)        # suradnice vzhladom na PC
dim(food.pca$x)
summary(food.pca)
print(summary(food.pca)$importance)
# Ako by sme mohli interpretovat a nazvat prve dva hlavne komponenty?
round(food.pca$rotation, 2)
# Vykreslenie:
plot(food.pca$x[,1:2], cex=0.5, pch=16) # manualne
biplot(food.pca, cex=c(0.5,1))          # aj s vztahmi s povodnymi premennymi
biplot(food.pca, cex=c(2,1), xlabs=rep(".", nrow(X)))  # bez cisel
library(ggplot2)
library(corrplot) ## corrplot
library(cluster) ## clustering algorithms
library(factoextra) ## clustering algorithms &visualization
library(mclust)
###Wholesale customers data
setwd("C:\\Users\\Thu Beo\\Desktop\\4leto\\VSas2\\Thu_Project_VSAS2")
wholesale  <- read.table("data.txt", header=TRUE)
head(wholesale)
attach(wholesale)
wholesale$Channel = as.factor(wholesale$Channel)
wholesale$Region = as.factor(wholesale$Region)
summary(wholesale)
# Test normality:
qqnorm(Fresh)  # QQ-plot
qqline(Fresh)
ks.test(Fresh,"pnorm", mean=0,sd=sqrt(var(Fresh))) # Kolmogorov-Smirnovov test
shapiro.test(Fresh) #shapiro test
shapiro.test(Fresh) #shapiro test
qqnorm(Milk) # QQ-plot
qqline(Milk)
ks.test(Milk,"pnorm", mean=0,sd=sqrt(var(Milk))) # Kolmogorov-Smirnovov test
shapiro.test(Milk)
qqnorm(Grocery) # QQ-plot
qqline(Grocery)
ks.test(Grocery,"pnorm", mean=0,sd=sqrt(var(Milk))) # Kolmogorov-Smirnovov test
shapiro.test(Grocery)
#Visualizing data
round(cor(wholesale, 4))
plot(wholesale)
plot(wholesale[, 3:8], cex=0.7, pch=19)
#Data Preprocessing###
print(apply(wholesale, 2, function (x) sum(is.na(x)))) #Check missing values
wholesale1  <- wholesale
wholesale1 <- na.omit(wholesale1) #elimate missing values
wholesale1$Channel <- NULL #Delete feature Channel
wholesale1$Region <- NULL  #Delete feature Region
#applying log for each feature
wholesale1.log <- wholesale1
wholesale1.log$Fresh <- log(wholesale1.log$Fresh)
wholesale1.log$Milk <- log(wholesale1.log$Milk)
wholesale1.log$Grocery <- log(wholesale1.log$Grocery)
wholesale1.log$Frozen <- log(wholesale1.log$Frozen)
wholesale1.log$Det_Pap <- log(wholesale1.log$Det_Pap)
wholesale1.log$Delicassen <- log(wholesale1.log$Delicassen)
plot(wholesale1.log)
#Standardize the variables
good_data <- scale(wholesale1.log)
PCA <- prcomp(good_data, scale = TRUE)
print(PCA)
summary(PCA)
print(summary(PCA)$importance)
atribute <- dim(good_data)
plot(0:atribute[2], c(0, summary(PCA)$importance[3,])*100, type="b", ylim=c(0, 100), xlab = "Dimensions", ylab = "Percentage of explained variances", main = "Scree plot")
# Graf do 2D:
plot(PCA$x[,1:2], cez=0.5, pch=16)
biplot(PCA, cex=c(0.5,1))
biplot(PCA, cex=c(2,1), xlabs=rep(".", nrow(X)))
plot(x, y, pch = 19, xlab = "1. component", ylab = "2. component", xlim=c(-4,6), ylim=c(-6,4))
par(mfrow=c(1, 2))
x <- t(PCA$x[, 1])
y <- t(PCA$x[, 2])
plot(x, y, pch = 19, xlab = "1. component", ylab = "2. component", xlim=c(-4,6), ylim=c(-6,4))
biplot(PCA, cex=c(0.5,1))
biplot(PCA, cex=c(2,1), xlabs=rep(".", nrow(X)))
plot(PCA$x[,1:2], cez=0.5, pch=19, xlab = "1. component", ylab = "2. component")
biplot(PCA, cex=c(0.5,1))
biplot(PCA, cex=c(2,1), xlabs=rep(".", nrow(X)))
plot(PCA$x[,1:2], cez=0.5, pch=19)
biplot(PCA, cex=c(0.5,1))
biplot(PCA, cex=c(2,1), xlabs=rep(".", nrow(X)))
PCA <- prcomp(good_data, scale = TRUE)
print(PCA)
summary(PCA)
print(summary(PCA)$importance)
atribute <- dim(good_data)
plot(0:atribute[2], c(0, summary(PCA)$importance[3,])*100, type="b", ylim=c(0, 100), xlab = "Dimensions", ylab = "Percentage of explained variances", main = "Scree plot")
# Graf do 2D:
plot(PCA$x[,1:2], cez=0.5, pch=19)
biplot(PCA, cex=c(0.5,1))
biplot(PCA, cex=c(2,1), xlabs=rep(".", nrow(X)))
library(plot3D)
z <- t(PCA$x[, 3])
scatter3D(PCA$x[,1:3], pch = 19, xlab = "1. component ", ylab = "2. component", zlab = "3. component")
biplot(PCA, cex=c(0.5,1))
biplot(PCA, cex=c(2,1), xlabs=rep(".", nrow(X)))
library(plot3D)
scatter3D(PCA$x[,1:3], pch = 19, xlab = "1. component ", ylab = "2. component", zlab = "3. component")
scatter3D(PCA$x[,1:3], pch = 19)
PCA
library(plot3D)
scatter3D(PCA$x[,1:3], pch = 19)
# data predelene vahou
X <- food[,9:14]
head(X)
# krajsie nazvy stlpcov
colnames(X) <- c("Fat", "Energy", "Carbohydrates", "Protein", "Cholesterol", "Saturated.fat")
# vyskalovanie
X <- X - tcrossprod(rep(1,n), apply(X,2,mean))  # vycentrovanie
Y <- as.matrix(X) %*% diag( 1/apply(X,2,sd) )   # disperzie na 1
colnames(Y) <- colnames(X)
# PCA
#----
P <- prcomp(Y, scale=FALSE)
plot(P$x[, 1], P$x[, 2])
biplot(P, cex=c(2,1), xlabs=rep(".", nrow(X)))
library(princurve)
pc <- principal_curve(Y, maxit=50)
plot(pc)            # krivka
points(pc, cex=0.8) # projekcie dat
# toto okno nezatvarat
pc
# aj s povodnymi datami
windows()
s <- pc$s       # suradnice projekcii
ord <- pc$ord   # usporiadanie bodov na krivke
vyber <- c(1,2) # ktore 2 premenne
plot(Y[,vyber], cex=0.5, pch=19)  # povodne data
points(s[ord,vyber], cex=0.8, col="red")
lines(s[ord,vyber ], col="red")
lam <- pc$lambda   # vzdialenosti projekcii od zaciatku krivky
plot(lam, rep(1,n))
# vyfarbene podla napr. kalorii
plot(lam[order(Y[,2])], rep(1,n), cex=0.7, col=heat.colors(n)[n:1], main="Kalorie")
# order usporiada od najmensieho po najvacsie
# heat color treba prehodit, aby cervena bola najvyssia hodnota
# pre 1-rozmer mozeme aj vykreslit hodnoty premennych
plot(lam[order(Y[,1])], food[order(Y[,1]),9], cex=0.8, pch=19,
ylab="Tuky", col=heat.colors(n)[n:1])
# pre 1-rozmer mozeme aj vykreslit hodnoty premennych
plot(lam[order(Y[,1])], food[order(Y[,1]),9], cex=0.8, pch=19,
ylab="Tuky", col=heat.colors(n)[n:1])
# x-ova os: pozicia na P krivke
# y-ova os: podiel tukov
plot(lam[order(Y[,2])], food[order(Y[,2]),10], cex=0.8, pch=19,
ylab="Kalorie", col=heat.colors(n)[n:1])
plot(lam[order(Y[,3])], food[order(Y[,3]),11], cex=0.8, pch=19,
ylab="Uhlovodiky", col=heat.colors(n)[n:1])
plot(lam[order(Y[,4])], food[order(Y[,4]),12], cex=0.8, pch=19,
ylab="Bielkoviny", col=heat.colors(n)[n:1])
plot(lam[order(Y[,5])], food[order(Y[,5]),13], cex=0.8, pch=19,
ylab="Cholesterol", col=heat.colors(n)[n:1])
plot(lam[order(Y[,6])], food[order(Y[,6]),14], cex=0.8, pch=19,
ylab="Nasytene tuky", col=heat.colors(n)[n:1])
# vsetko spolu
par(mfrow=c(3,2))
plot(lam[order(Y[,1])], food[order(Y[,1]),9], cex=0.8, pch=19,
ylab="Tuky", col=heat.colors(n)[n:1])
plot(lam[order(Y[,2])], food[order(Y[,2]),10], cex=0.8, pch=19,
ylab="Kalorie", col=heat.colors(n)[n:1])
plot(lam[order(Y[,3])], food[order(Y[,3]),11], cex=0.8, pch=19,
ylab="Uhlovodiky", col=heat.colors(n)[n:1])
plot(lam[order(Y[,4])], food[order(Y[,4]),12], cex=0.8, pch=19,
ylab="Bielkoviny", col=heat.colors(n)[n:1])
plot(lam[order(Y[,5])], food[order(Y[,5]),13], cex=0.8, pch=19,
ylab="Cholesterol", col=heat.colors(n)[n:1])
plot(lam[order(Y[,6])], food[order(Y[,6]),14], cex=0.8, pch=19,
ylab="Nasytene tuky", col=heat.colors(n)[n:1])
library(HSAUR)
data(voting)
print(voting)
n <- ncol(voting)
D <- voting  # matica nepodobnosti
cl=c("red","red","blue","blue","red","red","red","blue","blue","blue",
"blue","red","red","blue","blue")
# strany senatorov
# Klasicke skalovanie
#--------------------
vot.cmd <- cmdscale(D, k=2, eig=TRUE)
plot(vot.cmd$points, pch=16, cex=1.5, col=cl, main="Klasicke MS")
# aj s menami
plot(vot.cmd$points, xlim=range(vot.cmd$points[,1]) * 1.2, ylim=range(vot.cmd$points[,2]) * 1.2,
main="Klasicke MS", pch=16, col=cl)
text(vot.cmd$points[,1], vot.cmd$points[,2] + 0.4, labels=colnames(voting), col=cl, cex=1.2)
# Volba rozmeru
(lam <- vot.cmd$eig)
explained <- rep(0,9)
for(t in 1:9)
explained[t] <- sum(lam[1:t]) / sum(abs(lam))
plot(1:9, explained, pch=16, main="Volba rozmeru")
